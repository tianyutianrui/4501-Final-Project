{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmath\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmath\u001b[39;00m \u001b[39mimport\u001b[39;00m sin, cos, sqrt, atan2, radians\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbs4\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# all import statements needed for the project\n",
    "from math import *\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import numpy as np\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import warnings\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants to refer to any local data\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d52c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Location ID to Longitude and Latitude\n",
    "import geopandas as gpd\n",
    "df= gpd.read_file(\"taxi_zones/taxi_zones.shp\")\n",
    "df=df.to_crs(epsg=4326)\n",
    "df['Center_point']=df['geometry'].centroid\n",
    "df[\"long\"]=df.Center_point.map(lambda p:p.x)\n",
    "df[\"lat\"]=df.Center_point.map(lambda p:p.y)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a34a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let object id start from 1\n",
    "df.index[df['OBJECTID'] == 57].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "Not all data source we have with distance, so we calculate distance for those source without distance and add it to our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(from_coord, to_coord):\n",
    "    \"\"\"\n",
    "    Calculate the distance between two coordinates\n",
    "    \"\"\"\n",
    "    x0 = from_coord[0]\n",
    "    y0 = from_coord[1]\n",
    "    x1 = to_coord[0]\n",
    "    y1 = to_coord[1]\n",
    "    R = 6373.0\n",
    "    lat0 = radians(y0)\n",
    "    lon0 = radians(x0)\n",
    "    lat1 = radians(y1)\n",
    "    lon1 = radians(x1)\n",
    "    dlon = lon1 - lon0\n",
    "    dlat = lat1 - lat0\n",
    "    a = sin(dlat / 2)**2 + cos(lat0) * cos(lat1) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(df):\n",
    "    \"\"\"\n",
    "    Add the distance calculated to dataframe\n",
    "    \"\"\"\n",
    "    from_coord = [\"pickup_longitude\",\"pickup_latitude\"]\n",
    "    to_coord = [\"dropoff_longitude\",\"dropoff_latitude\"]\n",
    "    df['distance'] = np.nan\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index,'distance'] = calculate_distance([row[\"pickup_longitude\"],row[\"pickup_latitude\"]],[row[\"dropoff_longitude\"],row[\"dropoff_latitude\"]])                                       \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4674e67a",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "Download, clean, and and sample Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "def get_taxi_html(TAXI_URL):\n",
    "    \"\"\"\n",
    "    Get taxi data from website\n",
    "    \"\"\"\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b26190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_parquet_links(TAXI_URL):\n",
    "    \"\"\"\n",
    "    Find parquet link with yellow taxi from html\n",
    "    \"\"\"\n",
    "    soup=bs4.BeautifulSoup(get_taxi_html(TAXI_URL), 'html.parser')\n",
    "    res=[]\n",
    "    par=soup.find_all('a')\n",
    "    for i in par:\n",
    "        if 'Yellow Taxi Trip Records' in i:\n",
    "            res.append(i.get('href'))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c09912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_taxi_csv_urls(TAXI_URL):\n",
    "    \"\"\"\n",
    "    Find URLs in a string\n",
    "    \"\"\"\n",
    "    return find_taxi_parquet_links(TAXI_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(data_path):\n",
    "    \"\"\"\n",
    "    Read parquet files in pandas\n",
    "    \"\"\"\n",
    "    return pd.read_parquet(data_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e52b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(df,obj,sub):\n",
    "    \"\"\"\n",
    "    Merge parquets files into a dateframe\n",
    "    \"\"\"\n",
    "    for index, row in df.iterrows():\n",
    "        for name in sub:\n",
    "            if df.loc[index,name] == df.loc[index,name]:\n",
    "                df.loc[index,obj] = df.loc[index,name]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "def get_and_clean_taxi_data(TAXI_URL,df):\n",
    "    \"\"\"\n",
    "    Download, clean, and and sample Taxi dataframe by removing invalid data and normalizing column names\n",
    "    \"\"\"\n",
    "    taxi_data = pd.DataFrame(columns=['tpep_pickup_datetime', 'tpep_dropoff_datetime','passenger_count', 'trip_distance', \n",
    "           'PULocationID', 'DOLocationID', 'fare_amount','tip_amount', 'tolls_amount', \n",
    "           'total_amount','pickup_longitude','pickup_latitude',  'dropoff_longitude', 'dropoff_latitude']) \n",
    "    all_csv_urls = find_taxi_csv_urls(TAXI_URL)\n",
    "    for csv_url in all_csv_urls:\n",
    "        file_name = re.search('https://d37ci6vzurychx.cloudfront.net/trip-data/(.*).parquet', csv_url).group(1)\n",
    "        if  int(file_name[-7:-3])<2015 or (int(file_name[-7:-3])==2015 and int(file_name[-2:])<=6):\n",
    "            if os.path.isfile('./'+file_name+'.parquet'):\n",
    "            # maybe: first try to see if you've downloaded this exact\n",
    "            # file already and saved it before trying again\n",
    "                print(file_name,\" saved\")\n",
    "                dataframe = pd.read_parquet(file_name+'.parquet')\n",
    "                #dataframe = dataframe.sample(n=100, random_state=1)\n",
    "            else:\n",
    "                print(file_name,\" downloading\")\n",
    "                dataframe = get_and_clean_month_taxi_data(csv_url)\n",
    "                dataframe = dataframe.sample(n=3000, random_state=1)\n",
    "                dataframe.to_parquet(file_name+'.parquet')\n",
    "            if int(file_name[-7:-3])==2010:\n",
    "                dataframe.rename(columns={\"pickup_datetime\": \"tpep_pickup_datetime\", \"dropoff_datetime\": \"tpep_dropoff_datetime\"},inplace=True)\n",
    "            elif int(file_name[-7:-3])==2009:\n",
    "                dataframe.rename(columns={\"Trip_Pickup_DateTime\": \"tpep_pickup_datetime\", \"Trip_Dropoff_DateTime\": \"tpep_dropoff_datetime\",\n",
    "                                         \"Passenger_Count\":\"passenger_count\",\"Trip_Distance\": \"trip_distance\", \n",
    "                                         \"Start_Lon\": \"pickup_longitude\",\"Start_Lat\":\"pickup_latitude\",\n",
    "                                         \"End_Lon\": \"dropoff_longitude\",\"End_Lat\":\"dropoff_latitude\",\n",
    "                                         \"Fare_Amt\": \"fare_amount\",\"Tip_Amt\":\"tip_amount\",\n",
    "                                         \"Tolls_Amt\": \"tolls_amount\",\"Total_Amt\":\"total_amount\"},inplace=True)\n",
    "            else:\n",
    "                for index,row in dataframe.iterrows():\n",
    "                    SID = int(dataframe.loc[index,'PULocationID'])\n",
    "                    EID = int(dataframe.loc[index,'DOLocationID'])\n",
    "                    if SID<=263 and EID<=263 and SID>=1 and EID>=1:\n",
    "                        S_index = df.index[df['OBJECTID'] == SID].values[0]\n",
    "                        E_index = df.index[df['OBJECTID'] == EID].values[0]\n",
    "                        dataframe.loc[index,'pickup_longitude'] = df.loc[S_index,'long']\n",
    "                        dataframe.loc[index,'pickup_latitude'] = df.loc[S_index,'lat']\n",
    "                        dataframe.loc[index,'dropoff_longitude'] = df.loc[E_index,'long']\n",
    "                        dataframe.loc[index,'dropoff_latitude'] = df.loc[E_index,'lat']\n",
    "                    else:\n",
    "                        dataframe.drop(index=index,inplace=True)\n",
    "            taxi_data = taxi_data.append(dataframe)\n",
    "            taxi_data = taxi_data[['tpep_pickup_datetime', 'tpep_dropoff_datetime','passenger_count', 'trip_distance', \n",
    "           'PULocationID', 'DOLocationID', 'fare_amount','tip_amount', 'tolls_amount', \n",
    "           'total_amount','pickup_longitude','pickup_latitude',  'dropoff_longitude', 'dropoff_latitude']]         \n",
    "    taxi_data.fillna(0,inplace=True)\n",
    "    return taxi_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "Download and clean Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    \"\"\"\n",
    "    Download and clean Uber dataframe by removing invalid data and normalizing column names\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(csv_file)\n",
    "    # remove trips outside of the constraint\n",
    "    data = data[(data['pickup_longitude']>=-74.242330) & (data['pickup_longitude']<=-73.717047) & \n",
    "                (data['pickup_latitude']>=40.560445) & (data['pickup_latitude']<=40.908524) &\n",
    "                (data['dropoff_longitude']>=-74.242330) & (data['dropoff_longitude']<=-73.717047) & \n",
    "                (data['dropoff_latitude']>=40.560445) & (data['dropoff_latitude']<=40.908524)]\n",
    "    # select useful columns\n",
    "    data = data[['fare_amount','pickup_datetime','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']]\n",
    "    # change to the correct datatype\n",
    "    data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'], format='%Y-%m-%d %H:%M:%S %Z')\n",
    "    data['pickup_datetime'] = data['pickup_datetime'].dt.tz_localize(None)\n",
    "    \n",
    "    data.fillna(0,inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data(UBER_DATA):\n",
    "    \"\"\"\n",
    "    Save the clean dataframe\n",
    "    \"\"\"\n",
    "    if os.path.isfile('./'+'uber_dataframe.xlsx'):\n",
    "        uber_dataframe = pd.read_excel('uber_dataframe.xlsx')\n",
    "    else:\n",
    "        uber_dataframe = load_and_clean_uber_data(UBER_DATA)\n",
    "        add_distance_column(uber_dataframe)\n",
    "        uber_dataframe.to_excel('uber_dataframe.xlsx')\n",
    "    return uber_dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "Download and clean Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    \"\"\"\n",
    "    Download and clean hourly weather data\n",
    "    \"\"\"\n",
    "    all_df = pd.read_csv(csv_file+'.csv')\n",
    "    # select useful columns\n",
    "    all_df = all_df[['STATION','DATE','LATITUDE','LONGITUDE','NAME','HourlyWindSpeed','HourlyPrecipitation']]\n",
    "    # change to the correct datatype\n",
    "    all_df['DATE'] = pd.to_datetime(all_df['DATE'], format='%Y-%m-%d %H:%M:%S')\n",
    "    # deal with missing data\n",
    "    all_df.fillna(0,inplace=True)\n",
    "    mask = all_df['HourlyPrecipitation'] == \"T\"\n",
    "    all_df.loc[mask, 'HourlyPrecipitation'] = 0\n",
    "    all_df['HourlyPrecipitation'] = all_df['HourlyPrecipitation'].map(lambda x: str(x).rstrip('s'))\n",
    "    all_df['HourlyPrecipitation'] = all_df['HourlyPrecipitation'].astype(\"float64\")\n",
    "    # add new columns for classification\n",
    "    all_df['day'] = all_df['DATE'].dt.date\n",
    "    all_df['hour'] = all_df['DATE'].dt.hour\n",
    "    # classification grouped by hour and date\n",
    "    all_df = all_df.groupby([all_df['day'],all_df['hour']]).agg({'HourlyWindSpeed':'mean', 'HourlyPrecipitation':'sum'})\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    \"\"\"\n",
    "    Download and clean daily weather data\n",
    "    \"\"\"\n",
    "    all_df = pd.read_csv(csv_file+'.csv')\n",
    "    # select useful columns\n",
    "    all_df = all_df[['STATION','DATE','LATITUDE','LONGITUDE','NAME','HourlyWindSpeed','HourlyPrecipitation']]\n",
    "    # change to the correct datatype\n",
    "    all_df['DATE'] = pd.to_datetime(all_df['DATE'], format='%Y-%m-%d %H:%M:%S')\n",
    "    # deal with missing data\n",
    "    all_df.fillna(0,inplace=True)\n",
    "    mask = all_df['HourlyPrecipitation'] == \"T\"\n",
    "    all_df.loc[mask, 'HourlyPrecipitation'] = 0\n",
    "    all_df['HourlyPrecipitation'] = all_df['HourlyPrecipitation'].map(lambda x: str(x).rstrip('s'))\n",
    "    all_df['HourlyPrecipitation'] = all_df['HourlyPrecipitation'].astype(\"float64\")\n",
    "    # add new columns for classification\n",
    "    all_df['day'] = all_df['DATE'].dt.date\n",
    "    # classification grouped by hour and date\n",
    "    all_df = all_df.groupby([all_df['day']]).agg({'HourlyWindSpeed':'mean', 'HourlyPrecipitation':'sum'})\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    \"\"\"\n",
    "    Save clean weather data to dataframe \n",
    "    \"\"\"\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    # add some way to find all weather CSV files\n",
    "    # or just add the name/paths manually\n",
    "    weather_csv_files = ['2009_weather','2010_weather','2011_weather','2012_weather','2013_weather','2014_weather','2015_weather']  \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)    \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "Execute all the required functions to download and save our clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e63ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_and_clean_taxi_data(TAXI_URL,df)\n",
    "uber_data = get_uber_data(UBER_CSV)\n",
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84292d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc1a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.to_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22256c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60519c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d073acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a978d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e10408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "Create tables to store cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your 4 tables\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "   hourId INTEGER PRIMARY KEY,\n",
    "   day DATE,\n",
    "   hour INTEGER,\n",
    "   HourlyWindSpeed FLOAT,\n",
    "   HourlyPrecipitation FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "   dayId INTEGER PRIMARY KEY,\n",
    "   day DATE,\n",
    "   HourlyWindSpeed FLOAT,\n",
    "   HourlyPrecipitation FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "   taxiId INTEGER PRIMARY KEY,\n",
    "   tpep_pickup_datetime DATETIME,\n",
    "   tpep_dropoff_datetime DATETIME,\n",
    "   passenger_count INTEGER,\n",
    "   trip_distance FLOAT,\n",
    "   PULocationID INTEGER,\n",
    "   DOLocationID INTEGER,\n",
    "   fare_amount FLOAT,\n",
    "   tip_amount FLOAT,\n",
    "   tolls_amount FLOAT,\n",
    "   total_amount FLOAT,\n",
    "   pickup_longitude FLOAT,\n",
    "   pickup_latitude FLOAT,\n",
    "   dropoff_longitude FLOAT,\n",
    "   dropoff_latitude FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "   uberId INTEGER PRIMARY KEY,\n",
    "   fare_amount FLOAT,\n",
    "   pickup_datetime DATETIME,\n",
    "   pickup_longitude FLOAT,\n",
    "   pickup_latitude FLOAT,\n",
    "   dropoff_longitude FLOAT,\n",
    "   dropoff_latitude FLOAT,\n",
    "   passenger_count INTEGER,\n",
    "   distance FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(HOURLY_WEATHER_SCHEMA)\n",
    "    connection.execute(DAILY_WEATHER_SCHEMA)\n",
    "    connection.execute(TAXI_TRIPS_SCHEMA)\n",
    "    connection.execute(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "Add data to the table we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    \"\"\"\n",
    "    Add dataframes to the table we just created\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        for table_name in [ \"taxi_trips\", \"uber_trips\", \"hourly_weather\", \"daily_weather\"]:\n",
    "            current_df = table_to_df_dict[table_name]\n",
    "            for i in tqdm(range(len(current_df))):\n",
    "                data_peice = [current_df.iloc[i][key] for key in current_df.columns]\n",
    "                #print(tuple(data_peice))\n",
    "                for j in range(len(data_peice)):\n",
    "                    if isinstance(data_peice[j], datetime.date):\n",
    "                        #print(\"triggered\")\n",
    "                        sql_date = str(data_peice[j])\n",
    "                        data_peice[j] = sql_date\n",
    "                        pass\n",
    "                data_peice = [i+1] + data_peice\n",
    "                sql_command = 'insert into {} values {};'.format(table_name, tuple(data_peice))\n",
    "                # print(sql_command)\n",
    "                connection.execute(sql_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process entire session\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "cursor = session.execute(\"select * from uber_trips\")\n",
    "result = cursor.fetchall()\n",
    "session.close()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6214185",
   "metadata": {},
   "outputs": [],
   "source": [
    "del uber_data['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data_table_name = uber_data \n",
    "taxi_data_table_name = taxi_data\n",
    "hour_weather_table_name = hourly_weather_data\n",
    "daily_weather_table_name = daily_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77263a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query, outfile):\n",
    "    with open(outfile,'w') as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "The most popular hour of the day to take a yellow taxi from 01-2009 to 06-2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "SELECT STRFTIME('%H', tpep_pickup_datetime) as hour, COUNT(*) as ORDERPERHOUR\n",
    "FROM taxi_trips\n",
    "GROUP BY STRFTIME('%H', tpep_pickup_datetime)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24183542",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = engine.execute(QUERY_1).fetchall()\n",
    "# write_query_to_file(QUERY_N, \"query_1.sql\")\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "beca7117",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "The most popular day of the week to take an uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = \"\"\"\n",
    "SELECT STRFTIME('%w', pickup_datetime) as weekday,\n",
    "       COUNT(*) as ORDERWEEKDAY\n",
    "FROM uber_trips\n",
    "GROUP BY STRFTIME('%w', pickup_datetime)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da990d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = engine.execute(QUERY_2).fetchall()\n",
    "# write_query_to_file(QUERY_N, \"query_2.sql\")\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "281c7035",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "The 95% percentile of distance traveled for all hired trips during July 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ae0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3 = \"\"\"\n",
    "SELECT hired_data.distance AS '95% distance'\n",
    "FROM \n",
    "(\n",
    "SELECT tpep_pickup_datetime as pickup_datetime, trip_distance as distance \n",
    "FROM taxi_trips WHERE STRFTIME('%Y', tpep_pickup_datetime) = '2013' AND STRFTIME('%m', tpep_pickup_datetime) = '07'\n",
    "UNION ALL\n",
    "SELECT pickup_datetime,distance \n",
    "FROM uber_trips WHERE STRFTIME('%Y', pickup_datetime) = '2013' AND STRFTIME('%m', pickup_datetime) = '07'\n",
    ") as hired_data  \n",
    "ORDER BY hired_data.distance ASC\n",
    "LIMIT 1\n",
    "OFFSET (SELECT\n",
    "         COUNT(*)\n",
    "        FROM (\n",
    "            SELECT tpep_pickup_datetime as pickup_datetime, trip_distance as distance \n",
    "            FROM taxi_trips WHERE STRFTIME('%Y', tpep_pickup_datetime) = '2013' AND STRFTIME('%m', tpep_pickup_datetime) = '07'\n",
    "            UNION ALL\n",
    "            SELECT pickup_datetime, distance \n",
    "            FROM uber_trips WHERE STRFTIME('%Y', pickup_datetime) = '2013' AND STRFTIME('%m', pickup_datetime) = '07'\n",
    "             ) as hired_data  \n",
    "        ) * 95 / 100 - 1;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c213ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = engine.execute(QUERY_3).fetchall()\n",
    "# write_query_to_file(QUERY_N, \"query_3.sql\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e356d923",
   "metadata": {},
   "source": [
    "### Query 4\n",
    "The top 10 days with the highest number of hired rides for 2009\n",
    "The average distance for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4 = \"\"\"\n",
    "SELECT hired_data.pickup_datetime, COUNT(*), AVG(hired_data.distance) \n",
    "FROM \n",
    "(\n",
    "    SELECT tpep_pickup_datetime as pickup_datetime, trip_distance as distance \n",
    "    FROM taxi_trips WHERE STRFTIME('%Y', tpep_pickup_datetime) = '2009'\n",
    "    UNION ALL\n",
    "    SELECT pickup_datetime, distance \n",
    "    FROM uber_trips WHERE STRFTIME('%Y', pickup_datetime) = '2009'\n",
    ")as hired_data GROUP BY STRFTIME('%j', hired_data.pickup_datetime) \n",
    "HAVING COUNT(*) IN (  SELECT COUNT(*)\n",
    "    FROM \n",
    "    (\n",
    "        SELECT tpep_pickup_datetime as pickup_datetime, trip_distance as distance \n",
    "        FROM taxi_trips WHERE STRFTIME('%Y', tpep_pickup_datetime) = '2009'\n",
    "        UNION ALL\n",
    "        SELECT pickup_datetime, distance \n",
    "        FROM uber_trips WHERE STRFTIME('%Y', pickup_datetime) = '2009'\n",
    "    )as hired_data \n",
    "    GROUP BY STRFTIME('%j', hired_data.pickup_datetime)\n",
    "    ORDER BY count(*) DESC\n",
    "    LIMIT 10);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a48c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=engine.execute(QUERY_4).fetchall()\n",
    "# write_query_to_file(QUERY_N, \"query_4.sql\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b820e8",
   "metadata": {},
   "source": [
    "### Query 5\n",
    "The windiest 10 days in 2014\n",
    "Number of hired trips were made on those days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "QQUERY_5 = \"\"\"\n",
    "\n",
    "SELECT hired_data.pickup_datetime, COUNT(*) \n",
    "\n",
    "FROM(\n",
    "        SELECT tpep_pickup_datetime as pickup_datetime, trip_distance as distance \n",
    "        FROM taxi_trips WHERE STRFTIME('%Y', tpep_pickup_datetime) = '2014'\n",
    "        UNION ALL\n",
    "        SELECT pickup_datetime, distance \n",
    "        FROM uber_trips WHERE STRFTIME('%Y', pickup_datetime) = '2014'\n",
    ")\n",
    "as hired_data \n",
    "INNER JOIN daily_weather\n",
    "ON STRFTIME('%Y', hired_data.pickup_datetime) = STRFTIME('%Y', daily_weather.day) \n",
    "AND STRFTIME('%j', hired_data.pickup_datetime) = STRFTIME('%j', daily_weather.day) \n",
    "GROUP BY STRFTIME('%j', hired_data.pickup_datetime) \n",
    "ORDER BY daily_weather.HourlyWindSpeed  DESC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50e297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=engine.execute(QUERY_5).fetchall()\n",
    "# write_query_to_file(QUERY_N, \"query_5.sql\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7253850",
   "metadata": {},
   "source": [
    "### Query 6\n",
    "Hurricane Sandy in NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e5371",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6 = \"\"\"\n",
    "SELECT COUNT(*),hired_data.pickup_datetime, HourlyPrecipitation, HourlyWindSpeed  \n",
    "FROM \n",
    "(\n",
    "    SELECT tpep_pickup_datetime as pickup_datetime\n",
    "    FROM taxi_trips WHERE STRFTIME('%Y', tpep_pickup_datetime) = '2012' AND STRFTIME('%m', pickup_datetime) = '10'\n",
    "                            AND STRFTIME('%d', tpep_pickup_datetime) <= '30'  AND STRFTIME('%d', tpep_pickup_datetime) >= '22'\n",
    "    UNION ALL\n",
    "    SELECT pickup_datetime\n",
    "    FROM uber_trips WHERE STRFTIME('%Y', pickup_datetime) = '2012' AND STRFTIME('%m', pickup_datetime) = '10'\n",
    "        AND STRFTIME('%d', pickup_datetime) <= '30'  AND STRFTIME('%d', pickup_datetime) >= '22'\n",
    ") as hired_data\n",
    "INNER JOIN hourly_weather\n",
    "ON STRFTIME('%d', hired_data.pickup_datetime) = STRFTIME('%d', hourly_weather.day) \n",
    "AND STRFTIME('%H', hired_data.pickup_datetime) = hourly_weather.hour\n",
    "GROUP BY STRFTIME('%d', hired_data.pickup_datetime), STRFTIME('%H', hired_data.pickup_datetime);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13810e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=engine.execute(QUERY_6).fetchall()\n",
    "# write_query_to_file(QUERY_N, \"query_6.sql\")\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each visualization._\n",
    "\n",
    "_The example below makes use of the `matplotlib` library. There are other libraries, including `pandas` built-in plotting library, kepler for geospatial data representation, `seaborn`, and others._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1(engine):\n",
    "    df = pd.DataFrame(engine.execute(QUERY_1).fetchall())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    values = [item[1] for item in pickup_per_hour.values.tolist()]  # use the dataframe to pull out values needed to plot\n",
    "    x = np.arange(len(list(values)))\n",
    "\n",
    "    plt.bar(x, values,color=(76/255,114/255,176/255),edgecolor=\"white\",alpha=1, label='num. of trips',zorder=10)\n",
    "#     plt.yscale(\"log\")\n",
    "    plt.ylabel(\"Number of yellow taxi trips taken\", labelpad=10.0)\n",
    "    plt.xlabel(\"hour in a day\", labelpad=10.0)\n",
    "    plt.xticks(np.arange(24))\n",
    "#     plt.ylim((0, 1600))\n",
    "\n",
    "    axes.set_title(\"Visualization of Query 1 in Part 3\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_per_hour = get_data_for_visual_1(engine)\n",
    "plot_visual_1(pickup_per_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_2(engine):\n",
    "    \n",
    "    QUERY_VISUAL_2 = \"\"\"    \n",
    "    SELECT hired_data.month, hired_data.distance\n",
    "    FROM (\n",
    "    SELECT STRFTIME(\"%m\",pickup_datetime) as month, distance FROM uber_trips\n",
    "    UNION ALL\n",
    "    SELECT STRFTIME(\"%m\",tpep_pickup_datetime) as month, trip_distance as distance FROM taxi_trips\n",
    "    ) as hired_data\n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(engine.execute(QUERY_VISUAL_2).fetchall())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03988d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard_deviation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0196189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_distance_month = get_data_for_visual_2(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b411e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_distance_month.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002483ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_distance_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_distance = {}\n",
    "for i in range(0,12):\n",
    "    monthly_distance[i]= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in pickup_distance_month.values.tolist():\n",
    "    monthly_distance[int(row[0])-1].append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e7a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 独立计算每个月份的confidence？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61422080",
   "metadata": {},
   "outputs": [],
   "source": [
    "means, conf_ls, conf_rs = [], [], []\n",
    "for key, data in monthly_distance.items():\n",
    "    import scipy.stats as st\n",
    "    m = np.mean(data)\n",
    "    #create 90% confidence interval for population mean weight\n",
    "    l, r = st.norm.interval(alpha=0.90, loc=np.mean(data), scale=st.sem(data))\n",
    "    means.append(m)\n",
    "    conf_ls.append(l)\n",
    "    conf_rs.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b73e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_2(means, conf_ls, conf_rs):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = means  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    x=np.arange(12)\n",
    "    axes.plot(x,values)\n",
    "    axes.fill_between(x, conf_ls, conf_rs, color='b', alpha=.1)\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    plt.ylabel(\"Distance: km\", labelpad=10.0)\n",
    "    plt.xticks(np.arange(12),['January','February','March','April','May','June','July','August','September','October','November','December'])\n",
    "    axes.set_title(\"average distance traveled per month\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e50ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_visual_2(means, conf_ls, conf_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_5(engine):\n",
    "    # uber trip中没有 tip amount\n",
    "    QUERY_VISUAL_2 = \"\"\"    \n",
    "    SELECT trip_distance as distance, tip_amount \n",
    "    FROM taxi_trips\n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(engine.execute(QUERY_VISUAL_2).fetchall())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a623399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_5(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    distance = [row[0] for row in dataframe.values.tolist()]\n",
    "    tip = [row[1] for row in dataframe.values.tolist()]\n",
    "    plt.scatter(x=distance,y=tip, s=5)\n",
    "    plt.xlabel(\"Trip Distance: km\", labelpad=10.0)\n",
    "    plt.ylabel(\"Amount of tips: $\", labelpad=10.0)\n",
    "    axes.set_title(\"Relationship between tip and distance\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_distance = get_data_for_visual_5(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b5f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_visual_5(tip_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70c8c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_6(engine):\n",
    "    # uber trip中没有 tip amount\n",
    "    QUERY_VISUAL_6 = \"\"\"    \n",
    "    SELECT hired_data.tip_amount, HourlyPrecipitation\n",
    "    FROM (\n",
    "        SELECT tpep_pickup_datetime as pickup_datetime, tip_amount\n",
    "        FROM taxi_trips\n",
    "    ) as hired_data\n",
    "\n",
    "    INNER JOIN hourly_weather\n",
    "    ON STRFTIME('%j', hired_data.pickup_datetime) = STRFTIME('%j', hourly_weather.day) \n",
    "    AND STRFTIME('%H', hired_data.pickup_datetime) = hourly_weather.hour\n",
    "    ;\n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(engine.execute(QUERY_VISUAL_6).fetchall())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da4321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_weather = get_data_for_visual_6(engine)\n",
    "tip_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef91da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_6(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    tip = [row[0] for row in dataframe.values.tolist()]\n",
    "    precipitation = [row[1] for row in dataframe.values.tolist()]\n",
    "    plt.scatter(x=precipitation,y=tip, s=5)\n",
    "    plt.xlabel(\"Hourly Precipitation\", labelpad=10.0)\n",
    "    plt.ylabel(\"Amount of tips: $\", labelpad=10.0)\n",
    "    axes.set_title(\"Relationship between tip and distance\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_visual_6(tip_weather)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
